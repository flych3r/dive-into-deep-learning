{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l.tensorflow import data, losses, optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter, vocab = data.load_seq_data('timemachine.txt', 128, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.one_hot(np.array([0, 2, 20]), len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Converting an entire minibatch to one-hot encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_onehot(X, size):\n",
    "    return [tf.one_hot(x, size) for x in tf.transpose(X)]\n",
    "\n",
    "X = np.arange(10).reshape((2, 5))\n",
    "inputs = to_onehot(X, len(vocab))\n",
    "print(len(inputs), inputs[0].shape)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "corpus_indices = data_iter.corpus\n",
    "idx_to_char, char_to_idx = vocab.idx_to_token, vocab.token_to_idx\n",
    "num_inputs, num_hiddens, num_outputs = vocab_size, 512, vocab_size\n",
    "\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        return tf.random.normal(stddev=0.01, shape=shape)\n",
    "    # Hidden layer parameters\n",
    "    W_xh = _one((num_inputs, num_hiddens))\n",
    "    W_hh = _one((num_hiddens, num_hiddens))\n",
    "    b_h = tf.zeros(num_hiddens)\n",
    "    # Output layer parameters\n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = tf.zeros(num_outputs)\n",
    "    # Attach a gradient\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    return [tf.Variable(param) for param in params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return tuples such that we can extend this later\n",
    "def init_rnn_state(batch_size, num_hiddens):\n",
    "    state = [tf.zeros(shape=(batch_size, num_hiddens)), ]# return tuples such that we can extend this \n",
    "    return [tf.Variable(st) for st in state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "    # Both inputs and outputs are composed of num_steps matrices\n",
    "    # of the shape (batch_size, vocab_size).\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        H = tf.tanh(tf.matmul(X, W_xh) + tf.matmul(H, W_hh) + b_h)\n",
    "        Y = tf.matmul(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs, (H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state = init_rnn_state(X.shape[0], num_hiddens)\n",
    "inputs = to_onehot(X, vocab_size)\n",
    "params = get_params()\n",
    "outputs, state_new = rnn(inputs, state, params)\n",
    "print(len(inputs), inputs[0].shape, state[0].shape)\n",
    "print(len(outputs), outputs[0].shape, state_new[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(\n",
    "    prefix, num_chars, rnn, params, init_rnn_state,\n",
    "    num_hiddens, vocab_size, idx_to_char, char_to_idx\n",
    "):\n",
    "    state = init_rnn_state(1, num_hiddens)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        # The output of the previous time step is taken\n",
    "        # as the input of the current time step.\n",
    "        X = to_onehot([output[-1]], vocab_size)\n",
    "        # Calculate the output and update the hidden state.\n",
    "        (Y, state) = rnn([X], state, params)\n",
    "        # The input to the next time step is the character in 3\n",
    "        # the prefix or the current best predicted character.\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            # This is maximum likelihood decoding, not sampling\n",
    "            output.append(int(tf.argmax(Y[0], axis=1)))\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_rnn(\n",
    "    'traveller', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size, idx_to_char, char_to_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(gradients, theta):\n",
    "    gradients = [tf.Variable(grad) for grad in gradients]\n",
    "    norm = tf.Variable(0.)\n",
    "    for grad in gradients:\n",
    "        norm.assign_add(tf.reduce_sum(grad ** 2))\n",
    "        norm.assign(tf.sqrt(norm))\n",
    "    if norm > theta:\n",
    "        for grad in gradients:\n",
    "            grad.assign_add(grad * (theta / norm))\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is saved in the d2l package for future use.\n",
    "def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                          vocab_size, corpus_indices, idx_to_char,\n",
    "                          char_to_idx, is_random_iter, num_epochs, num_steps,\n",
    "                          lr, clipping_theta, batch_size, pred_period,\n",
    "                          pred_len, prefixes):\n",
    "    import time\n",
    "    import math\n",
    "    from tqdm import tqdm\n",
    "    if is_random_iter:\n",
    "        data_iter_fn = data.seq_data_iter_random\n",
    "    else:\n",
    "        data_iter_fn = data.seq_data_iter_consecutive\n",
    "    params = get_params()\n",
    "    loss = losses.softmax_cross_entropy\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        if not is_random_iter:  \n",
    "            # If adjacent sampling is used, the hidden state is initialized \n",
    "            # at the beginning of the epoch.\n",
    "            state = init_rnn_state(batch_size, num_hiddens)\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps)\n",
    "        for X, Y in data_iter:\n",
    "            if is_random_iter:  \n",
    "                # If random sampling is used, the hidden state is initialized \n",
    "                # before each mini-batch update.\n",
    "                state = init_rnn_state(batch_size, num_hiddens)\n",
    "            # else:  \n",
    "            #     # Otherwise, the detach function needs to be used to separate \n",
    "            #     # the hidden state from the computational graph to avoid \n",
    "            #     # backpropagation beyond the current sample.\n",
    "            #     for s in state:\n",
    "            #         s.detach()\n",
    "            with tf.GradientTape() as t:\n",
    "                inputs = to_onehot(X, vocab_size)\n",
    "                # outputs is num_steps terms of shape (batch_size, vocab_size)\n",
    "                (outputs, state) = rnn(inputs, state, params)\n",
    "                # after stitching it is (num_steps * batch_size, vocab_size).\n",
    "                outputs = tf.concat(outputs, axis=0)\n",
    "                # The shape of Y is (batch_size, num_steps), and then becomes \n",
    "                # a vector with a length of batch * num_steps after \n",
    "                # transposition. This gives it a one-to-one correspondence \n",
    "                # with output rows.\n",
    "                y = tf.reshape(tf.transpose(Y), (-1,))\n",
    "                # Average classification error via cross entropy loss.\n",
    "                l = tf.reduce_mean(loss(y, outputs))\n",
    "            gradients = t.gradient(l, params)\n",
    "            gradients = grad_clipping(gradients, clipping_theta)  # Clip the gradient.\n",
    "            optimizers.sgd(params, gradients, lr, 1)  \n",
    "            # Since the error is the mean, no need to average gradients here.\n",
    "            l_sum += l * y.shape[0]\n",
    "            n += y.shape[0]\n",
    "\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "                epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_rnn(\n",
    "                    prefix, pred_len, rnn, params, init_rnn_state,\n",
    "                    num_hiddens, vocab_size, idx_to_char, char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 500, 64, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 50, 50, ['traveller', 'time traveller']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_and_predict_rnn(\n",
    "    rnn, get_params, init_rnn_state, num_hiddens,\n",
    "    vocab_size, corpus_indices, idx_to_char,\n",
    "    char_to_idx, True, num_epochs, num_steps, lr,\n",
    "    clipping_theta, batch_size, pred_period, pred_len,\n",
    "    prefixes\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594558666074",
   "display_name": "Python 3.7.7 64-bit ('d2l': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}