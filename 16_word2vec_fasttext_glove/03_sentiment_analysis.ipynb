{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:36:50.290407Z",
     "start_time": "2019-04-18T18:36:49.391263Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import tarfile\n",
    "from d2l.tensorflow.data import Vocab, batch_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:06.846180Z",
     "start_time": "2019-04-18T18:36:50.292250Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "23"
    }
   },
   "outputs": [],
   "source": [
    "fname = '../data/aclImdb_v1.tar.gz'\n",
    "data_dir = '../data/'\n",
    "with tarfile.open(fname, 'r') as f:\n",
    "    f.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:07.727569Z",
     "start_time": "2019-04-18T18:37:06.848226Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "24"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_imdb(folder='train'):\n",
    "    data, labels = [], []\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder_name = os.path.join(data_dir, 'aclImdb', folder, label)\n",
    "        for file in os.listdir(folder_name):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '')\n",
    "                data.append(review)\n",
    "                labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "train_data, test_data = read_imdb('train'), read_imdb('test')\n",
    "print('# trainings:', len(train_data[0]), '\\n# tests:', len(test_data[0]))\n",
    "for x, y in zip(train_data[0][:3], train_data[1][:3]):\n",
    "    print('label:', y, 'review:', x[0:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization and Vocabulary \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:10.750284Z",
     "start_time": "2019-04-18T18:37:07.729641Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "28"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    return [line.split(' ') for line in sentences]\n",
    "\n",
    "train_tokens = tokenize(train_data[0])\n",
    "test_tokens = tokenize(test_data[0])\n",
    "\n",
    "vocab = Vocab([tk for line in train_tokens for tk in line], min_freq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Padding to the Same Length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:20.118289Z",
     "start_time": "2019-04-18T18:37:10.751950Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "44"
    }
   },
   "outputs": [],
   "source": [
    "max_len = 500\n",
    "\n",
    "def pad(x):\n",
    "    if len(x) > max_len:        \n",
    "        return x[:max_len]\n",
    "    else:\n",
    "        return x + [vocab.unk] * (max_len - len(x))\n",
    "    \n",
    "train_features = np.array([pad(vocab[line]) for line in train_tokens])\n",
    "test_features = np.array([pad(vocab[line]) for line in test_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:20.123639Z",
     "start_time": "2019-04-18T18:37:20.119937Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_steps = len(train_features) // batch_size\n",
    "test_steps = len(test_features) // batch_size\n",
    "\n",
    "train_iter = batch_iter(train_features, train_data[1], batch_size)\n",
    "test_iter = batch_iter(test_features, test_data[1], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:20.138763Z",
     "start_time": "2019-04-18T18:37:20.125156Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use a Bidirectional LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, embed_size, num_hiddens, num_layers = len(vocab), 50, 100, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Embedding(vocab_size, embed_size))\n",
    "\n",
    "for i in range(num_layers - 1):\n",
    "    model.add(keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(num_hiddens, return_sequences=True))\n",
    "    )\n",
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(num_hiddens)))\n",
    "\n",
    "model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load Pre-trained Word Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector = {}\n",
    "\n",
    "with open('../data/glove.6B.50d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        value = line.split(' ')\n",
    "        word = value[0]\n",
    "        coef = np.array(value[1:], dtype=np.float32)\n",
    "        embedding_vector[word] = coef\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embed_size))\n",
    "\n",
    "for word, i in vocab.token_to_idx.items():\n",
    "    embedding_value = embedding_vector.get(word)\n",
    "    if embedding_value is not None:\n",
    "        embedding_matrix[i] = embedding_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:40.206184Z",
     "start_time": "2019-04-18T18:37:25.449068Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].weights[0][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:40.210859Z",
     "start_time": "2019-04-18T18:37:40.207714Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "47"
    }
   },
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].weights[0][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train and Evaluate the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:41:09.801349Z",
     "start_time": "2019-04-18T18:37:40.212205Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "48"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "model.fit(\n",
    "    train_iter, steps_per_epoch=train_steps, \n",
    "    validation_data=test_iter, validation_steps=test_steps,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predict sencences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:41:09.806821Z",
     "start_time": "2019-04-18T18:41:09.802933Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "49"
    }
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(net, vocab, sentence):\n",
    "    sentence = np.array(vocab[sentence.split()])\n",
    "    label = np.argmax(net(sentence.reshape((1, -1))), axis=1)\n",
    "    return 'positive' if label == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:41:09.814658Z",
     "start_time": "2019-04-18T18:41:09.808150Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "50"
    }
   },
   "outputs": [],
   "source": [
    "predict_sentiment(net, vocab, 'this movie is so great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:41:09.821180Z",
     "start_time": "2019-04-18T18:41:09.816015Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_sentiment(net, vocab, 'this movie is so bad')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('d2l': conda)",
   "language": "python",
   "name": "python_defaultSpec_1594835070035"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
